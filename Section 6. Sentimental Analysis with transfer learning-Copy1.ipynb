{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97546f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import cv2\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "import seaborn\n",
    "import datetime, pathlib, io, os, time, random, re, string\n",
    "import gensim.downloader as api\n",
    "from PIL import Image\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import Dense, Flatten, InputLayer, BatchNormalization, Input, Embedding, TextVectorization\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Bidirectional, Dropout, Conv1D\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy, TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d8eefa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df776b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-24 15:13:02.275144: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2023-10-24 15:13:02.275164: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2023-10-24 15:13:02.275168: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2023-10-24 15:13:02.275195: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-24 15:13:02.275212: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds = tfds.load('imdb_reviews', split=['train', 'test[:50%]', 'test[50%:]'], as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff85755",
   "metadata": {},
   "source": [
    "# Text Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8c8fe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    no_tag = tf.strings.regex_replace(lowercase, \"<[^>]+>\", \"\")\n",
    "    output = tf.strings.regex_replace(no_tag, \"[%s]\"%re.escape(string.punctuation), \"\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c5c109",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec3074b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000\n",
    "SEQUENCE_LENGTH = 250\n",
    "EMBEDDING_DIM=300\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "361a94c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "    standardize= standardization,\n",
    "    max_tokens = VOCAB_SIZE,\n",
    "    output_mode = 'int',\n",
    "    output_sequence_length = SEQUENCE_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e16106",
   "metadata": {},
   "source": [
    "### vocaburary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f10e45e4-cbe9-4355-b3e3-b8a74a989f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = train_ds.map(lambda x, y: tf.expand_dims(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dea9ec97-4b25-41b1-9e90-8c597dfdf322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"], shape=(1,), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-24 15:16:14.428859: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for i in training_data.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "150ca5cb-1edf-4e74-aa04-bdbf2082df79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b2cb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device('/cpu:0'):\n",
    "#     vectorize_layer.adapt(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce4249",
   "metadata": {},
   "source": [
    "### vectorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0f5f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(review, label):\n",
    "    return vectorize_layer(review), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd260a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_ds.map(vectorizer)\n",
    "val_dataset = val_ds.map(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8c5b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1fb8e4",
   "metadata": {},
   "source": [
    "# Pretrained Word2Vec(Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251dfe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a44244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b09cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec['The']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0de15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.most_similar('Man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6eff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_caps(word):\n",
    "    return word[0].upper() + word[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ff7a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = []\n",
    "\n",
    "for i in range(len(vectorize_layer.get_vocabulary())):\n",
    "    word = vectorize_layer.get_vocabulary()[i]\n",
    "    # print(f\"word is '{word}'\")\n",
    "    try:\n",
    "        pretrained_embeddings.append(word2vec[word])\n",
    "    except:\n",
    "        # print(f\"word is '{word}'\")\n",
    "        try:\n",
    "            pretrained_embeddings.append(word2vec[first_caps(word)])\n",
    "        except:\n",
    "            pretrained_embeddings.append(np.random.normal(loc=0, scale=1, size=(EMBEDDING_DIM)))\n",
    "        \n",
    "    if i%1000 == 0:\n",
    "        print(f\"====> i is {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2164d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings_array = np.array(pretrained_embeddings)\n",
    "pretrained_embeddings_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5af3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('data/pretraining_embeddings.npy', pretrained_embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bd5ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings_array = np.load('data/pretraining_embeddings.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da8eb20",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a000403",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    Input(shape=(SEQUENCE_LENGTH,)),\n",
    "    Embedding(\n",
    "        VOCAB_SIZE,\n",
    "        EMBEDDING_DIM,\n",
    "        embeddings_initializer=tf.keras.initializers.Constant(pretrained_embeddings_array),\n",
    "        trainable=False\n",
    "    ),\n",
    "    \n",
    "    Conv1D(32, 3, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a58a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'localdata/Section6/conv1d_word2vec.h5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447b7bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2b47b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=10,\n",
    "    callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0992be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model_loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5559bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model_accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b16b92",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f97b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e23c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_ds.map(vectorizer)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d377be",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4e0823",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices([['''This movie has always been a favorite of mine. I never like holiday movies, because i always find them to be full to bursting with slapstick comedy, or way too sugary-sweet and dramatic. both of these things are okay in moderation, but most Christmas movies seem to go to one side of the spectrum or the other. this wonderful fairy tale is perfect for someone like me, who likes a little bit of a darker movie, but expects a Christmas movie to have a good message. the darkness in the movie is not without cause-it shows the joy of Christmas in great contrast to the scariness of Halloween, and it made me love both holidays all the more for that reason. i don't know, maybe that's just because Halloween and Christmas are my favorite holidays, but i really feel that this movie is great for older children and adults. younger children (up to 5 or 6 years) may find this simply frightening, but older children would find it wonderful.'''],\n",
    "                                                  ['''`Ballistic: Ecks vs. Sever' has been saddled with not only one of the worst movie titles in recent memory, but one of the worst screenplays as well. The film's third-rate espionage plot makes no sense at all and serves basically as a lame excuse for endless explosions, shootouts and double-flipping car chases, which have become the standard accoutrements for virtually every action picture since `Bullitt' in 1968. The problem with `Ballistic' is that the viewer can never tell who is doing what to whom or why  and we never care. The film is really all about style anyway. How else to account for the rather ludicrous image of Lucy Liu - looking more like a fashion model out on a shoot than a trained killer doing the shooting herself - strolling in elegant slow motion through the streets of Vancouver, wiping out what seems to be an entire hit squad with a combination of superhuman marksmanship and Matrix-like kickboxing moves? With her ankle-length designer coat and her icy-cool demeanor, she looks like Calvin Klein's idea of what the well-dressed assassin should be wearing this season. It's enough to reduce the whole enterprise to the level of comic absurdity  and, indeed, I often found myself laughing out loud at many of the ostensibly serious shenanigans occurring in the film. The flashbacks, which are obviously intended to clarify the characters' relationships, are so poorly done that they actually end up making the whole story more muddled and confusing. (And, although the child-kidnapping scenario is never as offensive in this film as it is in `Trapped,' one can still question the propriety of filmmakers running to this theme with the kind of frequency they seem to have been doing of late).Antonio Banderas makes up the other half of the film's title (he is Ecks, she Sever), and one only wonders what he could have been thinking about when he signed on to co-star in this particular project. `Ballistic' is utterly dispensable moviemaking: here today, forgotten tomorrow, a film utterly without distinction, conviction or purpose.''']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09214b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer_test(review):\n",
    "    return vectorize_layer(review)\n",
    "test_dataset = test_dataset.map(vectorizer_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3df9e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8095e602",
   "metadata": {},
   "source": [
    "### Inference ready testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56511b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(1,), dtype=\"string\")\n",
    "vectorized_inputs = vectorize_layer(inputs)\n",
    "outputs = model(vectorized_inputs)\n",
    "interence_ready_model = tf.keras.Model(inputs, outputs)\n",
    "interence_ready_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ed738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "interence_ready_model.predict([\"This movie has always been a favorite of mine.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eaa245",
   "metadata": {},
   "source": [
    "# Visualizaing embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a633d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "model = tf.keras.models.Sequential([\n",
    "    Input(shape=(SEQUENCE_LENGTH,)),\n",
    "    Embedding(VOCAB_SIZE, EMBEDDING_DIM),\n",
    "    \n",
    "    Conv1D(32, 3, activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c797c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "             optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb95a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir='localdata/logs/imbd/fit/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b24e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e10b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=5,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4598e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(log_dir, 'metadata.tsv'), 'w', encoding='utf-8') as f:\n",
    "    for i in range(VOCAB_SIZE):\n",
    "        f.write(f\"{i} {vectorize_layer.get_vocabulary()[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1428efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = tf.Variable(model.layers[0].get_weights()[0])\n",
    "embedding_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2656e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.train.Checkpoint(embedding=embedding_weights)\n",
    "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
    "\n",
    "config = projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cefb543",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.metadata_path = 'metadata.tsv'\n",
    "projector.visualize_embeddings(log_dir, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae2a781",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fea0739",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir localdata/logs/imbd/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e6012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
