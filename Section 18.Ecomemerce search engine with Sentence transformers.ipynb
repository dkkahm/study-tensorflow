{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "97546f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tf.config.run_functions_eagerly(True)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import cv2\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "import seaborn\n",
    "import pathlib\n",
    "import io, os, re, string, time, datetime\n",
    "from numpy import random\n",
    "import gensim.downloader as api\n",
    "from PIL import Image\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras.layers import Dense, Flatten, InputLayer, BatchNormalization, Input, Embedding, TextVectorization\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Bidirectional, Dropout, Conv1D, MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy, TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorboard.plugins import projector\n",
    "import traceback\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TFBertForSequenceClassification, TFBertModel\n",
    "from transformers import create_optimizer, AutoTokenizer, TFAutoModel\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "82075b2a-88f9-4c8f-8973-515ba514e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00394de-18a0-4b9c-b9cb-0f6d5e1205a5",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2928069d-9717-444c-877c-6bba3266d30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_train = \"localdata/Section18/data/processed/public/task_1_query-product_ranking/train-v0.3.csv\"\n",
    "filepath_catalog = \"localdata/Section18/data/processed/public/task_1_query-product_ranking/product_catalogue-v0.3.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a12e4b3-1502-47b6-b735-8bc334be29ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_catalogue = pd.read_csv(filepath_catalog)\n",
    "df_train = pd.read_csv(filepath_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f29908ad-2704-4264-aeb2-ec5cd1af888f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(883868, 781744)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_catalogue), len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40184428-543e-4ca4-849a-5aa49ed1fb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>query_locale</th>\n",
       "      <th>product_id</th>\n",
       "      <th>esci_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td># 2 pencils not sharpened</td>\n",
       "      <td>us</td>\n",
       "      <td>B0000AQO0O</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td># 2 pencils not sharpened</td>\n",
       "      <td>us</td>\n",
       "      <td>B0002LCZV4</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td># 2 pencils not sharpened</td>\n",
       "      <td>us</td>\n",
       "      <td>B00125Q75Y</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td># 2 pencils not sharpened</td>\n",
       "      <td>us</td>\n",
       "      <td>B001AZ1D3C</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td># 2 pencils not sharpened</td>\n",
       "      <td>us</td>\n",
       "      <td>B001B097KC</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781739</th>\n",
       "      <td>33803</td>\n",
       "      <td>針なしほっちきす</td>\n",
       "      <td>jp</td>\n",
       "      <td>B08XGQ9RH7</td>\n",
       "      <td>substitute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781740</th>\n",
       "      <td>33803</td>\n",
       "      <td>針なしほっちきす</td>\n",
       "      <td>jp</td>\n",
       "      <td>B0987RGRF2</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781741</th>\n",
       "      <td>33803</td>\n",
       "      <td>針なしほっちきす</td>\n",
       "      <td>jp</td>\n",
       "      <td>B099NFJWP6</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781742</th>\n",
       "      <td>33803</td>\n",
       "      <td>針なしほっちきす</td>\n",
       "      <td>jp</td>\n",
       "      <td>B09F3B413J</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781743</th>\n",
       "      <td>33803</td>\n",
       "      <td>針なしほっちきす</td>\n",
       "      <td>jp</td>\n",
       "      <td>B09F3F1KDP</td>\n",
       "      <td>exact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>781744 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        query_id                      query query_locale  product_id  \\\n",
       "0              0  # 2 pencils not sharpened           us  B0000AQO0O   \n",
       "1              0  # 2 pencils not sharpened           us  B0002LCZV4   \n",
       "2              0  # 2 pencils not sharpened           us  B00125Q75Y   \n",
       "3              0  # 2 pencils not sharpened           us  B001AZ1D3C   \n",
       "4              0  # 2 pencils not sharpened           us  B001B097KC   \n",
       "...          ...                        ...          ...         ...   \n",
       "781739     33803                   針なしほっちきす           jp  B08XGQ9RH7   \n",
       "781740     33803                   針なしほっちきす           jp  B0987RGRF2   \n",
       "781741     33803                   針なしほっちきす           jp  B099NFJWP6   \n",
       "781742     33803                   針なしほっちきす           jp  B09F3B413J   \n",
       "781743     33803                   針なしほっちきす           jp  B09F3F1KDP   \n",
       "\n",
       "        esci_label  \n",
       "0            exact  \n",
       "1            exact  \n",
       "2            exact  \n",
       "3            exact  \n",
       "4            exact  \n",
       "...            ...  \n",
       "781739  substitute  \n",
       "781740       exact  \n",
       "781741       exact  \n",
       "781742       exact  \n",
       "781743       exact  \n",
       "\n",
       "[781744 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86a89cde-50e0-4b03-aa38-39eb31f0e5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_description</th>\n",
       "      <th>product_bullet_point</th>\n",
       "      <th>product_brand</th>\n",
       "      <th>product_color_name</th>\n",
       "      <th>product_locale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0188A3QRM</td>\n",
       "      <td>Amazon Basics Woodcased #2 Pencils, Unsharpene...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144 woodcase #2 HB pencils made from high-qual...</td>\n",
       "      <td>Amazon Basics</td>\n",
       "      <td>Yellow</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B075VXJ9VG</td>\n",
       "      <td>BAZIC Pencil #2 HB Pencils, Latex Free Eraser,...</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;BACK TO BAZIC&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Our go...</td>\n",
       "      <td>&amp;#11088; UN-SHARPENED #2 PREMIUM PENCILS. Each...</td>\n",
       "      <td>BAZIC Products</td>\n",
       "      <td>12-count</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B07G7F6JZ6</td>\n",
       "      <td>Emraw Pre Sharpened Round Primary Size No 2 Ju...</td>\n",
       "      <td>&lt;p&gt;&lt;b&gt;Emraw Pre-Sharpened #2 HB Wood Pencils -...</td>\n",
       "      <td>✓ PACK OF 8 NUMBER 2 PRESHARPENED BEGINNERS PE...</td>\n",
       "      <td>Emraw</td>\n",
       "      <td>Yellow</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B07JZJLHCF</td>\n",
       "      <td>Emraw Pre Sharpened Triangular Primary Size No...</td>\n",
       "      <td>&lt;p&gt;&lt;b&gt;Emraw Pre-Sharpened #2 HB Wood Pencils -...</td>\n",
       "      <td>✓ PACK OF 6 NUMBER 2 PRESHARPENED BEGINNERS PE...</td>\n",
       "      <td>Emraw</td>\n",
       "      <td>Yellow</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B07MGKC3DD</td>\n",
       "      <td>BIC Evolution Cased Pencil, #2 Lead, Gray Barr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Premium #2 HB lead pencils with break-resistan...</td>\n",
       "      <td>Design House</td>\n",
       "      <td>Gray</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883863</th>\n",
       "      <td>B094NN6T8V</td>\n",
       "      <td>アイリスプラザ コーナーソファ 3点セット 高反発 2人掛け 3人掛け ブラウン 394310</td>\n",
       "      <td>NaN</td>\n",
       "      <td>商品サイズ（cm）：幅約154×奥行約109×高さ約33（※組み合わせ時）\\n商品重量：約6...</td>\n",
       "      <td>アイリスプラザ(IRIS PLAZA)</td>\n",
       "      <td>ブラウン</td>\n",
       "      <td>jp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883864</th>\n",
       "      <td>B096RD95K9</td>\n",
       "      <td>Uping テーブル・ベッドの高さ調節が簡単にできる ベッドの高さをあげる足 4個セット 高...</td>\n",
       "      <td>B096RD95K9</td>\n",
       "      <td>サイズ:約16.5×11.7×8.2cm\\n総重量:約1.23kg\\n材質:ABSプラスチッ...</td>\n",
       "      <td>Uping</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883865</th>\n",
       "      <td>B099Z4HXH1</td>\n",
       "      <td>アイリスプラザ ソファ ソファー ローソファ ローソファー 1人掛け 1人用 幅約108cm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>商品サイズ(cm):幅約108×奥行約44×高さ約62\\n商品重量:約15kg\\n材質 表面...</td>\n",
       "      <td>アイリスプラザ(IRIS PLAZA)</td>\n",
       "      <td>グレー</td>\n",
       "      <td>jp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883866</th>\n",
       "      <td>B09B9QMMSF</td>\n",
       "      <td>足入れヒーター 足温器 電気足温器 1～6時間タイマー 丸洗い可能 省エネ あったか脚入れヒ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>【腰からスッポリ包む、一人様用のごたつ】 つま先・腰・お腹まですっぽり包む寝袋のようなデザイ...</td>\n",
       "      <td>タサイスク</td>\n",
       "      <td>ブラウン</td>\n",
       "      <td>jp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883867</th>\n",
       "      <td>B09CNNHRQ1</td>\n",
       "      <td>Mensu ノートパソコンデスク 竹製 折りたたみ式  勉強机 ローテーブル 書見台 ブック...</td>\n",
       "      <td>人体工学に基づいて、最新のデザイン。4段階の角度調整可能設計で、必要に応じて、勉強、仕事など...</td>\n",
       "      <td>【広い用途】折りたたみ式ノートパソコンデスク、勉強机、ブックスタンド、タブレットやスマホスタ...</td>\n",
       "      <td>棉素 Mensu</td>\n",
       "      <td>竹原色</td>\n",
       "      <td>jp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>883868 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        product_id                                      product_title  \\\n",
       "0       B0188A3QRM  Amazon Basics Woodcased #2 Pencils, Unsharpene...   \n",
       "1       B075VXJ9VG  BAZIC Pencil #2 HB Pencils, Latex Free Eraser,...   \n",
       "2       B07G7F6JZ6  Emraw Pre Sharpened Round Primary Size No 2 Ju...   \n",
       "3       B07JZJLHCF  Emraw Pre Sharpened Triangular Primary Size No...   \n",
       "4       B07MGKC3DD  BIC Evolution Cased Pencil, #2 Lead, Gray Barr...   \n",
       "...            ...                                                ...   \n",
       "883863  B094NN6T8V    アイリスプラザ コーナーソファ 3点セット 高反発 2人掛け 3人掛け ブラウン 394310   \n",
       "883864  B096RD95K9  Uping テーブル・ベッドの高さ調節が簡単にできる ベッドの高さをあげる足 4個セット 高...   \n",
       "883865  B099Z4HXH1  アイリスプラザ ソファ ソファー ローソファ ローソファー 1人掛け 1人用 幅約108cm...   \n",
       "883866  B09B9QMMSF  足入れヒーター 足温器 電気足温器 1～6時間タイマー 丸洗い可能 省エネ あったか脚入れヒ...   \n",
       "883867  B09CNNHRQ1  Mensu ノートパソコンデスク 竹製 折りたたみ式  勉強机 ローテーブル 書見台 ブック...   \n",
       "\n",
       "                                      product_description  \\\n",
       "0                                                     NaN   \n",
       "1       <p><strong>BACK TO BAZIC</strong></p><p>Our go...   \n",
       "2       <p><b>Emraw Pre-Sharpened #2 HB Wood Pencils -...   \n",
       "3       <p><b>Emraw Pre-Sharpened #2 HB Wood Pencils -...   \n",
       "4                                                     NaN   \n",
       "...                                                   ...   \n",
       "883863                                                NaN   \n",
       "883864                                         B096RD95K9   \n",
       "883865                                                NaN   \n",
       "883866                                                NaN   \n",
       "883867  人体工学に基づいて、最新のデザイン。4段階の角度調整可能設計で、必要に応じて、勉強、仕事など...   \n",
       "\n",
       "                                     product_bullet_point  \\\n",
       "0       144 woodcase #2 HB pencils made from high-qual...   \n",
       "1       &#11088; UN-SHARPENED #2 PREMIUM PENCILS. Each...   \n",
       "2       ✓ PACK OF 8 NUMBER 2 PRESHARPENED BEGINNERS PE...   \n",
       "3       ✓ PACK OF 6 NUMBER 2 PRESHARPENED BEGINNERS PE...   \n",
       "4       Premium #2 HB lead pencils with break-resistan...   \n",
       "...                                                   ...   \n",
       "883863  商品サイズ（cm）：幅約154×奥行約109×高さ約33（※組み合わせ時）\\n商品重量：約6...   \n",
       "883864  サイズ:約16.5×11.7×8.2cm\\n総重量:約1.23kg\\n材質:ABSプラスチッ...   \n",
       "883865  商品サイズ(cm):幅約108×奥行約44×高さ約62\\n商品重量:約15kg\\n材質 表面...   \n",
       "883866  【腰からスッポリ包む、一人様用のごたつ】 つま先・腰・お腹まですっぽり包む寝袋のようなデザイ...   \n",
       "883867  【広い用途】折りたたみ式ノートパソコンデスク、勉強机、ブックスタンド、タブレットやスマホスタ...   \n",
       "\n",
       "              product_brand product_color_name product_locale  \n",
       "0             Amazon Basics             Yellow             us  \n",
       "1            BAZIC Products           12-count             us  \n",
       "2                     Emraw             Yellow             us  \n",
       "3                     Emraw             Yellow             us  \n",
       "4              Design House               Gray             us  \n",
       "...                     ...                ...            ...  \n",
       "883863  アイリスプラザ(IRIS PLAZA)               ブラウン             jp  \n",
       "883864                Uping                NaN             jp  \n",
       "883865  アイリスプラザ(IRIS PLAZA)                グレー             jp  \n",
       "883866                タサイスク               ブラウン             jp  \n",
       "883867             棉素 Mensu                竹原色             jp  \n",
       "\n",
       "[883868 rows x 7 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a84287f1-af80-49b1-a4b8-883197f1478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_title(product_id):\n",
    "    options = []\n",
    "    options.append(str(product_id))\n",
    "    # print(options)\n",
    "    matched_row = df_catalogue[df_catalogue['product_id'].isin(options)]\n",
    "    # print(matched_row)\n",
    "    # print(matched_row['product_title'])\n",
    "    # print(int(str(matched_row['product_title']).split(\"    \")[0]))\n",
    "    # return matched_row['product_title'][(int(str(matched_row['product_title']).split(\"    \")[0]))]\n",
    "    return matched_row['product_title'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed46405b-dbec-4546-a921-760e63cba812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ticonderoga Beginner Pencils, Wood-Cased #2 HB Soft, With Eraser, Yellow, 12-Pack (13308)'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_product_title('B0000AQO0O')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1093cff-5329-43aa-9640-cd30a0ca3467",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b0ff8ee5-aaca-4ed1-9c86-9c1c25430bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_train = 'localdata/Section18/csv_file_9.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4f918423-4d26-43fd-add2-a8cd38532811",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_train, df_catalogue, on='product_id').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e3256a18-331d-46cb-b923-91a6411a3320",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = df_merged[['query', 'product_title', 'esci_label']]\n",
    "df_dataset.columns = ['query', 'product', 'label']\n",
    "df_dataset.to_csv(filepath_train, header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6a5b5996-1681-4eb4-a79b-f18a207d940c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b51a20018346d78e9458d2d6452bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38cc298f84e64f3fa91888e1806ab837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7bfc60a4194e49aa144febe77cd893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('csv', data_files=[filepath_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3e375960-0218-40ec-9bfc-845d46ff5a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['query', 'product', 'label'],\n",
       "        num_rows: 236242\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4d05e264-0f18-4f93-ad7e-69c64e95f0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '# 2 pencils not sharpened',\n",
       " 'product': 'Arteza HB Pencils #2, Pack of 48, Wood-Cased Graphite Pencils in Bulk, Pre-Sharpened, with Latex-Free Erasers, Office & School Supplies for Exams and Classrooms',\n",
       " 'label': 'complement'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8acf4e4d-702f-49c7-b55d-68548cc65b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(label):\n",
    "    if label == 'exact':\n",
    "        return 1.0\n",
    "    elif label == 'substitute':\n",
    "        return 0.7\n",
    "    elif label == 'complement':\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "edd145d6-85bb-4ad9-a1d5-d72707a54946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a9fdabe9be485eae792f5a90920836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/383 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ffc59c09e5a4aeebc8066c3845304da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b90d85546e496eaf131d51595c8bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9d1ce8d4ea40f6862b79b0378d1305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = 'sentence-transformers/multi-qa-MiniLM-L6-cos-v1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "54b956a4-ec51-478b-beff-c57742044323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset):\n",
    "    if dataset['product'] == None:\n",
    "        dataset['product'] = dataset['query']\n",
    "\n",
    "    dataset['input_ids_query'] = []\n",
    "    dataset['token_type_ids_query'] = []\n",
    "    dataset['attention_mask_query'] = []\n",
    "\n",
    "    dataset['input_ids_product'] = []\n",
    "    dataset['token_type_ids_product'] = []\n",
    "    dataset['attention_mask_product'] = []\n",
    "\n",
    "    query_tokens = tokenizer(dataset['query'], max_length=MAX_LENGTH, padding='max_length', truncation=True)\n",
    "    dataset['input_ids_query'].append(query_tokens['input_ids'])\n",
    "    dataset['token_type_ids_query'].append(query_tokens['token_type_ids'])\n",
    "    dataset['attention_mask_query'].append(query_tokens['attention_mask'])\n",
    "\n",
    "    product_tokens = tokenizer(dataset['product'], max_length=MAX_LENGTH, padding='max_length', truncation=True)\n",
    "    dataset['input_ids_product'].append(product_tokens['input_ids'])\n",
    "    dataset['token_type_ids_product'].append(product_tokens['token_type_ids'])\n",
    "    dataset['attention_mask_product'].append(product_tokens['attention_mask'])\n",
    "\n",
    "    dataset['label'] = get_label(dataset['label'])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9af8c039-f69b-4be9-b3b2-0f34859ed124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cae8984c93645bbbbeeb1772f4250c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/236242 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prep_dataset = dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1113aac0-90fd-4432-863f-04c330c7ace3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '# 2 pencils not sharpened',\n",
       " 'product': 'Arteza HB Pencils #2, Pack of 48, Wood-Cased Graphite Pencils in Bulk, Pre-Sharpened, with Latex-Free Erasers, Office & School Supplies for Exams and Classrooms',\n",
       " 'label': 0.5,\n",
       " 'input_ids_query': [[101,\n",
       "   1001,\n",
       "   1016,\n",
       "   14745,\n",
       "   2015,\n",
       "   2025,\n",
       "   26694,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'token_type_ids_query': [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'attention_mask_query': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'input_ids_product': [[101,\n",
       "   16185,\n",
       "   4143,\n",
       "   1044,\n",
       "   2497,\n",
       "   14745,\n",
       "   2015,\n",
       "   1001,\n",
       "   1016,\n",
       "   1010,\n",
       "   5308,\n",
       "   1997,\n",
       "   4466,\n",
       "   1010,\n",
       "   3536,\n",
       "   1011,\n",
       "   2553,\n",
       "   2094,\n",
       "   10629,\n",
       "   4221,\n",
       "   14745,\n",
       "   2015,\n",
       "   1999,\n",
       "   9625,\n",
       "   1010,\n",
       "   3653,\n",
       "   1011,\n",
       "   26694,\n",
       "   1010,\n",
       "   2007,\n",
       "   2397,\n",
       "   2595,\n",
       "   1011,\n",
       "   2489,\n",
       "   22505,\n",
       "   2869,\n",
       "   1010,\n",
       "   2436,\n",
       "   1004,\n",
       "   2082,\n",
       "   6067,\n",
       "   2005,\n",
       "   13869,\n",
       "   1998,\n",
       "   12463,\n",
       "   102,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'token_type_ids_product': [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " 'attention_mask_product': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]]}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c73b37f1-78ec-43ff-a311-f899d038a88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 15:44:25.023306: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2023-11-06 15:44:25.023339: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2023-11-06 15:44:25.023344: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2023-11-06 15:44:25.023381: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-11-06 15:44:25.023399: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "tf_dataset = prep_dataset[\"train\"].to_tf_dataset(\n",
    "    columns=[\"input_ids_query\", \"token_type_ids_query\", \"attention_mask_query\", \"input_ids_product\", \"token_type_ids_product\", \"attention_mask_product\", \"label\"],\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a9b4666e-4d25-4091-9c8f-7bf7771e877f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
      "array([1. , 0.7, 0. , 0. , 0. , 1. , 1. , 0.7, 1. , 1. , 0.7, 1. , 0. ,\n",
      "       0.5, 1. , 1. , 1. , 1. , 0.7, 0.7, 1. , 0. , 1. , 0.7, 1. , 0.5,\n",
      "       1. , 1. , 1. , 1. , 0.5, 0.7], dtype=float32)>, 'input_ids_query': <tf.Tensor: shape=(32, 1, 64), dtype=int64, numpy=\n",
      "array([[[  101,  4689,  2519, ...,     0,     0,     0]],\n",
      "\n",
      "       [[  101,  1015,  1011, ...,     0,     0,     0]],\n",
      "\n",
      "       [[  101,  7473,  7492, ...,     0,     0,     0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[  101,  9088,  3422, ...,     0,     0,     0]],\n",
      "\n",
      "       [[  101,  9553, 12170, ...,     0,     0,     0]],\n",
      "\n",
      "       [[  101,  3336,  5302, ...,     0,     0,     0]]])>, 'token_type_ids_query': <tf.Tensor: shape=(32, 1, 64), dtype=int64, numpy=\n",
      "array([[[0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0]]])>, 'attention_mask_query': <tf.Tensor: shape=(32, 1, 64), dtype=int64, numpy=\n",
      "array([[[1, 1, 1, ..., 0, 0, 0]],\n",
      "\n",
      "       [[1, 1, 1, ..., 0, 0, 0]],\n",
      "\n",
      "       [[1, 1, 1, ..., 0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[1, 1, 1, ..., 0, 0, 0]],\n",
      "\n",
      "       [[1, 1, 1, ..., 0, 0, 0]],\n",
      "\n",
      "       [[1, 1, 1, ..., 0, 0, 0]]])>, 'input_ids_product': <tf.Tensor: shape=(32, 1, 64), dtype=int64, numpy=\n",
      "array([[[  101, 28407,  2009, ...,     0,     0,     0]],\n",
      "\n",
      "       [[  101, 25545,  2732, ...,     0,     0,     0]],\n",
      "\n",
      "       [[  101,  3968, 18095, ...,     0,     0,     0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[  101,  2128,  7096, ...,  1013,  1016,   102]],\n",
      "\n",
      "       [[  101, 15536,  5403, ...,     0,     0,     0]],\n",
      "\n",
      "       [[  101,  8840,  3630, ...,     0,     0,     0]]])>, 'token_type_ids_product': <tf.Tensor: shape=(32, 1, 64), dtype=int64, numpy=\n",
      "array([[[0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0]]])>, 'attention_mask_product': <tf.Tensor: shape=(32, 1, 64), dtype=int64, numpy=\n",
      "array([[[1, 1, 1, ..., 0, 0, 0]],\n",
      "\n",
      "       [[1, 1, 1, ..., 0, 0, 0]],\n",
      "\n",
      "       [[1, 1, 1, ..., 0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 0, 0, 0]],\n",
      "\n",
      "       [[1, 1, 1, ..., 0, 0, 0]]])>}\n"
     ]
    }
   ],
   "source": [
    "for i in tf_dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a96b32-7f51-4a80-b388-fefe02a0b2b5",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bd4677d8-7f1f-4f5b-a32b-2f47fa1728fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9bb274d64d844ebbba078f85c76edcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a2b1c609e94d7b8ce6389bdf0eb728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tf_model.h5:   0%|          | 0.00/91.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertModel.\n",
      "\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at sentence-transformers/multi-qa-MiniLM-L6-cos-v1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  22713216  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22713216 (86.64 MB)\n",
      "Trainable params: 22713216 (86.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModel.from_pretrained(model_id)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ac4c459d-c75f-49c1-b900-fd36a3832a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTransfomer(tf.keras.Model):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.dense = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def compile(self, optimizer, loss_fn):\n",
    "        super().compile()\n",
    "        self.loss_fn = loss_fn\n",
    "        self.loss_metric = tf.keras.metrics.Mean(name='loss')\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_metric]\n",
    "\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]\n",
    "\n",
    "        input_mask_expanded = tf.cast(\n",
    "            tf.broadcast_to(tf.expand_dims(attention_mask, -1), tf.shape(token_embeddings)),\n",
    "            tf.float32)\n",
    "        return tf.math.reduce_sum(token_embeddings + input_mask_expanded, axis=1) / \\\n",
    "            tf.clip_by_value(tf.math.reduce_sum(input_mask_expanded, axis=1), 1e-9, tf.float32.max)\n",
    "\n",
    "    def train_step(self, train_data):\n",
    "        query = {\n",
    "            'input_ids': train_data['input_ids_query'][:, 0, :],\n",
    "            'token_type_ids': train_data['token_type_ids_query'][:, 0, :],\n",
    "            'attention_mask': train_data['attention_mask_query'][:, 0, :]\n",
    "        }\n",
    "        product = {\n",
    "            'input_ids': train_data['input_ids_product'][:, 0, :],\n",
    "            'token_type_ids': train_data['token_type_ids_product'][:, 0, :],\n",
    "            'attention_mask': train_data['attention_mask_product'][:, 0, :]\n",
    "        }\n",
    "\n",
    "        labels = train_data['label']\n",
    "\n",
    "        with tf.GradientTape() as recorder:\n",
    "            query_predication = self.model(query)\n",
    "            pred_query = self.mean_pooling(query_predication, train_data['attention_mask_query'][:, 0, :])\n",
    "\n",
    "            product_predication = self.model(product)\n",
    "            pred_product = self.mean_pooling(product_predication, train_data['attention_mask_product'][:, 0, :])\n",
    "\n",
    "            pred_concat = tf.concat([pred_query, pred_product, tf.abs(pred_query - pred_product)], axis=-1)\n",
    "\n",
    "            predication = self.dense(pred_concat)\n",
    "            loss = self.loss_fn(labels, predication)\n",
    "\n",
    "        partial_derivatives = recorder.gradient(loss, self.model.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(partial_derivatives, self.model.trainable_weights))\n",
    "\n",
    "        self.loss_metric.update_state(loss)\n",
    "\n",
    "        return {'loss': self.loss_metric.result(), }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3122728-41e1-4e0e-b7ca-78f3bfe5b39e",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "86c59ba6-f1c6-4cea-9d73-5e82ed887d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "stransformer = SentenceTransfomer(model)\n",
    "stransformer.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(2e-5),\n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "51aac283-1f67-429b-99f2-a4f5105ed618",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "2023-11-06 16:24:52.198306: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1796/7383 [======>.......................] - ETA: 22:43 - loss: 0.7312"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mstransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.9/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1482\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1484\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1485\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "history = stransformer.fit(tf_dataset, epochs=EPOCHS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
